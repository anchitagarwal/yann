{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need for Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to start the discussion with occum's Razer which suggests us to choose the simplest model that works. Choosing a simple model for a neural network is difficult because it is inherently complex. A neural network learns the distribution of data while training so that it can work on new data in that distribution(test accuracy). There will be a slight performance drop between the training time and testing time and this drop is called generalization error. In few cases when the training is too aggressive, the network starts learning the data and starts fitting the data instead of learning the data distribution which results in a poor performance at test time. This is a result of poor generalizability of the network. We use some techniques to improve the generalization of a network and these techniques are also called as regularization. There are three main regularizations used in neural networks\n",
    "1. Classic Regularization ($L^1$ and $L^2$)\n",
    "2. Dropouts\n",
    "3. Batch Normalization\n",
    "\n",
    "## Classic Regularization(Weight Norm penalities):\n",
    "Regularization helps us to simplify our final model even with a complex architecture. One classic type of regularization is weight penalities which keeps the values of weight vectors in check. We achieve this we add the norm of the weight vector to the error function to get the final cost function. We can use any norm from $L^1$ to $L^\\infty$. The most widely used norms are $L^2$ and $L^1$. \n",
    "\n",
    "### $L^2$ Regularization\n",
    "$L^2$ Regularization is also called as Ridge Regression or Tikhonov regularization. Among the weight penalities $L^2$ is the most used weight penality. $L^2$ Regularization penalizes the bigger weights. We achieve regularization by adding square of $L^2$ norm to the cost function. mathematical representation of $L^2$ regularization is given by:\n",
    "$$Cost = E(X) + \\lambda \\parallel W \\parallel_2 ^ 2$$\n",
    "New Gradient g of the cost function $E(X)$ w.r.t to Weights w is given by:\n",
    "$$g = \\frac{\\partial E(X)}{\\partial W} + 2 \\lambda W$$\n",
    "\n",
    "$\\lambda$ is the regularization coefficient that can be used to control the level of regularization.\n",
    "\n",
    "### $L^1$ Regularization\n",
    "\n",
    "In $L^1$ Regularization we add the first norm of the weight vector to the cost function. $L^1$ Regularization penalizes the weights that are not zero. It forces the weights to be zero as a result of which the final parameters are sparse with most of the weights bring zero. Mathematical representation of $L^1$ regularization is given by:\n",
    "$$Cost = E(X) + \\lambda \\parallel W \\parallel_1$$\n",
    "New Gradient g of the cost function $E(X)$ w.r.t to Weights w is given by:\n",
    "$$g = \\frac{\\partial E(X)}{\\partial W} + \\lambda sign(W)$$\n",
    "\n",
    "#### combination of Norm penalities:\n",
    "\n",
    "We do not have to restrict ourselves to one weight Norm penality for a parameter. We can have a combination of more than one weight penalities. Our final model will be impacted by the properties of all the regularizers. For example, If we use both $L^1$ and $L^2$ weight penalities in our model then the cost function becomes\n",
    "$$Cost = E(X) + \\lambda_2 \\parallel W \\parallel_2 ^ 2 + \\lambda_1 \\parallel W \\parallel_1$$\n",
    "New Gradient g of the cost function $E(X)$ w.r.t to Weight vector W is given by:\n",
    "$$g = \\frac{\\partial E(X)}{\\partial W} + 2 \\lambda_2 W + \\lambda sign(W) $$\n",
    "\n",
    "#### Regularization by Norm Penalities in YANN:\n",
    "YANN has a flexibility of regularizing selected layer or an entire network. To regularize a layer, we should set the following arguments for ***`network.add_layer()`*** function\n",
    "<pre>\n",
    "regularize – True is you want to apply regularization, False if not.\n",
    "regularizer – coeffients for L1, L2 regulaizer coefficients,Default is (0.001, 0.001).\n",
    "</pre>\n",
    "To give common regularization parameters for entire network, we can give regularization argument for optimizer parameters.\n",
    " <pre>\"regularization\"    : (l1_coeff, l2_coeff). Default is (0.001, 0.001) </pre>\n",
    " \n",
    " Let's see Regularization in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Setting up dataset \n",
      ".. setting up skdata\n",
      "... Importing mnist from skdata\n",
      ".. setting up dataset\n",
      ".. training data\n",
      ".. validation data \n",
      ".. testing data \n",
      ". Dataset 78347 is created.\n",
      ". Time taken is 0.47504 seconds\n",
      ".. This method will be deprecated with the implementation of a visualizer,also this works only for tree-like networks. This will cause errors in printing DAG-style networks.\n",
      " |-\n",
      " |-\n",
      " |-\n",
      " |- id: input\n",
      " |-=================------------------\n",
      " |- type: input\n",
      " |- output shape: (500, 1, 28, 28)\n",
      " |------------------------------------\n",
      "          |-\n",
      "          |-\n",
      "          |-\n",
      "          |- id: conv_pool_1\n",
      "          |-=================------------------\n",
      "          |- type: conv_pool\n",
      "          |- output shape: (500, 20, 12, 12)\n",
      "          |- batch norm is OFF\n",
      "          |------------------------------------\n",
      "          |- filter size [5 X 5]\n",
      "          |- pooling size [2 X 2]\n",
      "          |- stride size [1 X 1]\n",
      "          |- input shape [28 28]\n",
      "          |- input number of feature maps is 1\n",
      "          |------------------------------------\n",
      "                   |-\n",
      "                   |-\n",
      "                   |-\n",
      "                   |- id: conv_pool_2\n",
      "                   |-=================------------------\n",
      "                   |- type: conv_pool\n",
      "                   |- output shape: (500, 50, 5, 5)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   |- batch norm is OFF\n",
      "                   |------------------------------------\n",
      "                   |- filter size [3 X 3]\n",
      "                   |- pooling size [2 X 2]\n",
      "                   |- stride size [1 X 1]\n",
      "                   |- input shape [12 12]\n",
      "                   |- input number of feature maps is 20\n",
      "                   |------------------------------------\n",
      "                            |-\n",
      "                            |-\n",
      "                            |-\n",
      "                            |- id: 4\n",
      "                            |-=================------------------\n",
      "                            |- type: flatten\n",
      "                            |- output shape: (500, 1250)\n",
      "                            |------------------------------------\n",
      "                                     |-\n",
      "                                     |-\n",
      "                                     |-\n",
      "                                     |- id: dot_product_1\n",
      "                                     |-=================------------------\n",
      "                                     |- type: dot_product\n",
      "                                     |- output shape: (500, 1250)\n",
      "                                     |- batch norm is OFF\n",
      "                                     |------------------------------------\n",
      "                                              |-\n",
      "                                              |-\n",
      "                                              |-\n",
      "                                              |- id: dot_product_2\n",
      "                                              |-=================------------------\n",
      "                                              |- type: dot_product\n",
      "                                              |- output shape: (500, 1250)\n",
      "                                              |- batch norm is OFF\n",
      "                                              |------------------------------------\n",
      "                                                       |-\n",
      "                                                       |-\n",
      "                                                       |-\n",
      "                                                       |- id: softmax\n",
      "                                                       |-=================------------------\n",
      "                                                       |- type: classifier\n",
      "                                                       |- output shape: (500, 10)\n",
      "                                                       |------------------------------------\n",
      "                                                                |-\n",
      "                                                                |-\n",
      "                                                                |-\n",
      "                                                                |- id: obj\n",
      "                                                                |-=================------------------\n",
      "                                                                |- type: objective\n",
      "                                                                |- output shape: (1,)\n",
      "                                                                |------------------------------------\n",
      ".. Cooking the network\n",
      ".. Setting up the resultor\n",
      ".. All checks complete, cooking continues\n",
      ".. Cost                : 1.65839\n",
      "... Learning Rate       : 9.99999974738e-05\n",
      "... Momentum            : 0.649999976158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.536438\n",
      "... Learning Rate       : 9.49999957811e-05\n",
      "... Momentum            : 0.660666584969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.301666\n",
      "... Learning Rate       : 9.02499959921e-05\n",
      "... Momentum            : 0.671333312988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.191394\n",
      "... Learning Rate       : 8.57374980114e-05\n",
      "... Momentum            : 0.681999981403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.135745\n",
      "... Learning Rate       : 8.14506202005e-05\n",
      "... Momentum            : 0.692666649818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.105225\n",
      "... Learning Rate       : 7.73780921008e-05\n",
      "... Momentum            : 0.703333318233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.0858618\n",
      "... Learning Rate       : 7.3509188951e-05\n",
      "... Momentum            : 0.713999986649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.0719565\n",
      "... Learning Rate       : 6.98337316862e-05\n",
      "... Momentum            : 0.724666655064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.0611352\n",
      "... Learning Rate       : 6.63420432829e-05\n",
      "... Momentum            : 0.735333323479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.0528243\n",
      "... Learning Rate       : 6.30249414826e-05\n",
      "... Momentum            : 0.745999991894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.0460229\n",
      "... Learning Rate       : 5.9873695136e-05\n",
      "... Momentum            : 0.756666600704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.040339\n",
      "... Learning Rate       : 5.68800096516e-05\n",
      "... Momentum            : 0.767333269119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.035487\n",
      "... Learning Rate       : 5.40360088053e-05\n",
      "... Momentum            : 0.777999937534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.0312105\n",
      "... Learning Rate       : 5.13342092745e-05\n",
      "... Momentum            : 0.788666665554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.0273663\n",
      "... Learning Rate       : 4.87674988108e-05\n",
      "... Momentum            : 0.799333274364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.0241477\n",
      "... Learning Rate       : 4.63291253254e-05\n",
      "... Momentum            : 0.80999994278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.021233\n",
      "... Learning Rate       : 4.40126677859e-05\n",
      "... Momentum            : 0.820666670799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.018664\n",
      "... Learning Rate       : 4.18120325776e-05\n",
      "... Momentum            : 0.83133327961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.016489\n",
      "... Learning Rate       : 3.97214316763e-05\n",
      "... Momentum            : 0.841999948025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:03                                                 \n",
      "| validation  100% Time: 0:00:01                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.0148402\n",
      "... Learning Rate       : 3.77353608201e-05\n",
      "... Momentum            : 0.85266661644\n"
     ]
    }
   ],
   "source": [
    "from yann.network import network\n",
    "from yann.utils.graph import draw_network\n",
    "from yann.special.datasets import cook_mnist\n",
    "def lenet5 ( dataset= None, verbose = 1, regularization = None ):             \n",
    "    \"\"\"\n",
    "    This function is a demo example of lenet5 from the infamous paper by Yann LeCun. \n",
    "    This is an example code. You should study this code rather than merely run it.  \n",
    "    \n",
    "    Warning:\n",
    "        This is not the exact implementation but a modern re-incarnation.\n",
    "\n",
    "    Args: \n",
    "        dataset: Supply a dataset.    \n",
    "        verbose: Similar to the rest of the dataset.\n",
    "    \"\"\"\n",
    "    optimizer_params =  {        \n",
    "                \"momentum_type\"       : 'nesterov',             \n",
    "                \"momentum_params\"     : (0.65, 0.97, 30),      \n",
    "                \"optimizer_type\"      : 'rmsprop',                \n",
    "                \"id\"                  : \"main\"\n",
    "                        }\n",
    "\n",
    "    dataset_params  = {\n",
    "                            \"dataset\"   : dataset,\n",
    "                            \"svm\"       : False, \n",
    "                            \"n_classes\" : 10,\n",
    "                            \"id\"        : 'data'\n",
    "                      }\n",
    "\n",
    "    visualizer_params = {\n",
    "                    \"root\"       : 'lenet5',\n",
    "                    \"frequency\"  : 1,\n",
    "                    \"sample_size\": 144,\n",
    "                    \"rgb_filters\": True,\n",
    "                    \"debug_functions\" : False,\n",
    "                    \"debug_layers\": False,  # Since we are on steroids this time, print everything.\n",
    "                    \"id\"         : 'main'\n",
    "                        }       \n",
    "\n",
    "    # intitialize the network\n",
    "    net = network(   borrow = True,\n",
    "                     verbose = verbose )                       \n",
    "    \n",
    "    # or you can add modules after you create the net.\n",
    "    net.add_module ( type = 'optimizer',\n",
    "                     params = optimizer_params, \n",
    "                     verbose = verbose )\n",
    "\n",
    "    net.add_module ( type = 'datastream', \n",
    "                     params = dataset_params,\n",
    "                     verbose = verbose )\n",
    "\n",
    "    net.add_module ( type = 'visualizer',\n",
    "                     params = visualizer_params,\n",
    "                     verbose = verbose \n",
    "                    )\n",
    "    # add an input layer \n",
    "    net.add_layer ( type = \"input\",\n",
    "                    id = \"input\",\n",
    "                    verbose = verbose, \n",
    "                    datastream_origin = 'data', # if you didnt add a dataset module, now is \n",
    "                                                 # the time. \n",
    "                    mean_subtract = False )\n",
    "    \n",
    "    # add first convolutional layer\n",
    "    net.add_layer ( type = \"conv_pool\",\n",
    "                    origin = \"input\",\n",
    "                    id = \"conv_pool_1\",\n",
    "                    num_neurons = 20,\n",
    "                    filter_size = (5,5),\n",
    "                    pool_size = (2,2),\n",
    "                    activation = 'relu',\n",
    "                    # regularize = True,\n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    net.add_layer ( type = \"conv_pool\",\n",
    "                    origin = \"conv_pool_1\",\n",
    "                    id = \"conv_pool_2\",\n",
    "                    num_neurons = 50,\n",
    "                    filter_size = (3,3),\n",
    "                    pool_size = (2,2),\n",
    "                    activation = 'relu',\n",
    "                    # regularize = True,\n",
    "                    verbose = verbose\n",
    "                    )      \n",
    "\n",
    "\n",
    "    net.add_layer ( type = \"dot_product\",\n",
    "                    origin = \"conv_pool_2\",\n",
    "                    id = \"dot_product_1\",\n",
    "                    num_neurons = 1250,\n",
    "                    activation = 'relu',\n",
    "                    # regularize = True,\n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    net.add_layer ( type = \"dot_product\",\n",
    "                    origin = \"dot_product_1\",\n",
    "                    id = \"dot_product_2\",\n",
    "                    num_neurons = 1250,                    \n",
    "                    activation = 'relu',  \n",
    "                    # regularize = True,    \n",
    "                    verbose = verbose\n",
    "                    ) \n",
    "    \n",
    "    net.add_layer ( type = \"classifier\",\n",
    "                    id = \"softmax\",\n",
    "                    origin = \"dot_product_2\",\n",
    "                    num_classes = 10,\n",
    "                    # regularize = True,\n",
    "                    activation = 'softmax',\n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    net.add_layer ( type = \"objective\",\n",
    "                    id = \"obj\",\n",
    "                    origin = \"softmax\",\n",
    "                    objective = \"nll\",\n",
    "                    datastream_origin = 'data', \n",
    "                    regularization = regularization,                \n",
    "                    verbose = verbose\n",
    "                    )\n",
    "                    \n",
    "    learning_rates = (0.05, .0001, 0.001)  \n",
    "    net.pretty_print()  \n",
    "    # draw_network(net.graph, filename = 'lenet.png')   \n",
    "\n",
    "    net.cook()\n",
    "\n",
    "    net.train( epochs = (20, 20), \n",
    "               validate_after_epochs = 1,\n",
    "               training_accuracy = True,\n",
    "               learning_rates = learning_rates,               \n",
    "               show_progress = True,\n",
    "               early_terminate = True,\n",
    "               patience = 2,\n",
    "               verbose = verbose)\n",
    "\n",
    "    net.test(verbose = verbose)\n",
    "data = cook_mnist()\n",
    "dataset = data.dataset_location()\n",
    "lenet5 ( dataset, verbose = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
