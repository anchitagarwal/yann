{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need for Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a traditional classroom, we have quizzes and exams. Quizzes test your learning and exams validate our knowledge gained in that course on the whole. However, the ultimate goal for a course is training a student to be able to apply that concept in other fields of career not just to get good marks in quizzes and exams. Similarly, we do not train a neural network to get amazing performance on the training set. We want a neural network perform well on entirely new data. Compared to training data performace, networks usually does not perform as well with new data. \n",
    "\n",
    "I want to start the discussion with occum's Razer which suggests us to choose the simplest model that works. Choosing a simple model for a neural network is difficult because it is inherently complex. A neural network learns the distribution of data while training so that it can work on new data in that distribution(test accuracy). There will be a slight performance drop between the training time and testing time and this drop is called generalization error. In few cases when the training is too aggressive, the network starts learning the data and starts fitting the data instead of learning the data distribution which results in a poor performance at test time. This is a result of poor generalizability of the network. We use some techniques to improve the generalization of a network and these techniques are also called as regularization. There are three main regularizations used in neural networks\n",
    "1. Classic Regularization ($L^1$ and $L^2$)\n",
    "2. Dropouts\n",
    "3. Batch Normalization\n",
    "\n",
    "## Classic Regularization(Weight Norm penalities):\n",
    "Regularization helps us to simplify our final model even with a complex architecture. One classic type of regularization is weight penalities which keeps the values of weight vectors in check. We achieve this we add the norm of the weight vector to the error function to get the final cost function. We can use any norm from $L^1$ to $L^\\infty$. The most widely used norms are $L^2$ and $L^1$. \n",
    "\n",
    "### $L^2$ Regularization\n",
    "$L^2$ Regularization is also called as Ridge Regression or Tikhonov regularization. Among the weight penalities $L^2$ is the most used weight penality. $L^2$ Regularization penalizes the bigger weights. We achieve regularization by adding square of $L^2$ norm to the cost function. mathematical representation of $L^2$ regularization is given by:\n",
    "$$Cost = E(X) + \\lambda \\parallel W \\parallel_2 ^ 2$$\n",
    "New Gradient g of the cost function $E(X)$ w.r.t to Weights w is given by:\n",
    "$$g = \\frac{\\partial E(X)}{\\partial W} + 2 \\lambda W$$\n",
    "\n",
    "$\\lambda$ is the regularization coefficient that can be used to control the level of regularization.\n",
    "\n",
    "### $L^1$ Regularization\n",
    "\n",
    "In $L^1$ Regularization we add the first norm of the weight vector to the cost function. $L^1$ Regularization penalizes the weights that are not zero. It forces the weights to be zero as a result of which the final parameters are sparse with most of the weights bring zero. Mathematical representation of $L^1$ regularization is given by:\n",
    "$$Cost = E(X) + \\lambda \\parallel W \\parallel_1$$\n",
    "New Gradient g of the cost function $E(X)$ w.r.t to Weights w is given by:\n",
    "$$g = \\frac{\\partial E(X)}{\\partial W} + \\lambda sign(W)$$\n",
    "\n",
    "#### combination of Norm penalities:\n",
    "\n",
    "We do not have to restrict ourselves to one weight Norm penality for a parameter. We can have a combination of more than one weight penalities. Our final model will be impacted by the properties of all the regularizers. For example, If we use both $L^1$ and $L^2$ weight penalities in our model then the cost function becomes\n",
    "$$Cost = E(X) + \\lambda_2 \\parallel W \\parallel_2 ^ 2 + \\lambda_1 \\parallel W \\parallel_1$$\n",
    "New Gradient g of the cost function $E(X)$ w.r.t to Weight vector W is given by:\n",
    "$$g = \\frac{\\partial E(X)}{\\partial W} + 2 \\lambda_2 W + \\lambda sign(W) $$\n",
    "\n",
    "#### Regularization by Norm Penalities in YANN:\n",
    "YANN has a flexibility of regularizing selected layer or an entire network. To regularize a layer, we should set the following arguments for ***`network.add_layer()`*** function\n",
    "<pre>\n",
    "regularize – True is you want to apply regularization, False if not.\n",
    "regularizer – coeffients for L1, L2 regulaizer coefficients,Default is (0.001, 0.001).\n",
    "</pre>\n",
    "To give common regularization parameters for entire network, we can give regularization argument for optimizer parameters.\n",
    " <pre>\"regularization\"    : (l1_coeff, l2_coeff). Default is (0.001, 0.001) </pre>\n",
    " \n",
    " Let's see Regularization in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "ERROR (theano.sandbox.cuda): ERROR: Not using GPU. Initialisation of device gpu failed:\n",
      "initCnmem: cnmemInit call failed! Reason=CNMEM_STATUS_OUT_OF_MEMORY. numdev=1\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cuda error: kernel_reduce_ccontig_node_544270fe7a21a748315f83abfe0913cc_0: out of memory. (grid: 1 x 1; block: 256 x 1 x 1)\n\nApply node that caused the error: GpuCAReduce{add}{1}(<CudaNdarrayType(float32, vector)>)\nToposort index: 0\nInputs types: [CudaNdarrayType(float32, vector)]\nInputs shapes: [(10000,)]\nInputs strides: [(1,)]\nInputs values: ['not shown']\nOutputs clients: [[HostFromGpu(GpuCAReduce{add}{1}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0332a69d7074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcook_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlenet5\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[0;32mbuild/bdist.linux-x86_64/egg/yann/network.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonyhunt/anaconda3/envs/python2/lib/python2.7/site-packages/theano/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_initial_driver_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_nvidia_driver1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m if (config.device.startswith('cuda') or\n",
      "\u001b[0;32m/home/tonyhunt/anaconda3/envs/python2/lib/python2.7/site-packages/theano/sandbox/cuda/tests/test_driver.pyc\u001b[0m in \u001b[0;36mtest_nvidia_driver1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m              'but got:'] + [str(app) for app in topo])\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         raise Exception(\"The nvidia driver version installed with this OS \"\n\u001b[1;32m     42\u001b[0m                         \u001b[0;34m\"does not give good results for reduction.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonyhunt/anaconda3/envs/python2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonyhunt/anaconda3/envs/python2/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonyhunt/anaconda3/envs/python2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cuda error: kernel_reduce_ccontig_node_544270fe7a21a748315f83abfe0913cc_0: out of memory. (grid: 1 x 1; block: 256 x 1 x 1)\n\nApply node that caused the error: GpuCAReduce{add}{1}(<CudaNdarrayType(float32, vector)>)\nToposort index: 0\nInputs types: [CudaNdarrayType(float32, vector)]\nInputs shapes: [(10000,)]\nInputs strides: [(1,)]\nInputs values: ['not shown']\nOutputs clients: [[HostFromGpu(GpuCAReduce{add}{1}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "from yann.network import network\n",
    "from yann.utils.graph import draw_network\n",
    "from yann.special.datasets import cook_mnist\n",
    "def lenet5 ( dataset= None, verbose = 1, regularization = None ):             \n",
    "    \"\"\"\n",
    "    This function is a demo example of lenet5 from the infamous paper by Yann LeCun. \n",
    "    This is an example code. You should study this code rather than merely run it.  \n",
    "    \n",
    "    Warning:\n",
    "        This is not the exact implementation but a modern re-incarnation.\n",
    "\n",
    "    Args: \n",
    "        dataset: Supply a dataset.    \n",
    "        verbose: Similar to the rest of the dataset.\n",
    "    \"\"\"\n",
    "    optimizer_params =  {        \n",
    "                \"momentum_type\"       : 'nesterov',             \n",
    "                \"momentum_params\"     : (0.65, 0.97, 30),      \n",
    "                \"optimizer_type\"      : 'rmsprop',                \n",
    "                \"id\"                  : \"main\"\n",
    "                        }\n",
    "\n",
    "    dataset_params  = {\n",
    "                            \"dataset\"   : dataset,\n",
    "                            \"svm\"       : False, \n",
    "                            \"n_classes\" : 10,\n",
    "                            \"id\"        : 'data'\n",
    "                      }\n",
    "\n",
    "    visualizer_params = {\n",
    "                    \"root\"       : 'lenet5',\n",
    "                    \"frequency\"  : 1,\n",
    "                    \"sample_size\": 144,\n",
    "                    \"rgb_filters\": True,\n",
    "                    \"debug_functions\" : False,\n",
    "                    \"debug_layers\": False,  # Since we are on steroids this time, print everything.\n",
    "                    \"id\"         : 'main'\n",
    "                        }       \n",
    "\n",
    "    # intitialize the network\n",
    "    net = network(   borrow = True,\n",
    "                     verbose = verbose )                       \n",
    "    \n",
    "    # or you can add modules after you create the net.\n",
    "    net.add_module ( type = 'optimizer',\n",
    "                     params = optimizer_params, \n",
    "                     verbose = verbose )\n",
    "\n",
    "    net.add_module ( type = 'datastream', \n",
    "                     params = dataset_params,\n",
    "                     verbose = verbose )\n",
    "\n",
    "    net.add_module ( type = 'visualizer',\n",
    "                     params = visualizer_params,\n",
    "                     verbose = verbose \n",
    "                    )\n",
    "    # add an input layer \n",
    "    net.add_layer ( type = \"input\",\n",
    "                    id = \"input\",\n",
    "                    verbose = verbose, \n",
    "                    datastream_origin = 'data', # if you didnt add a dataset module, now is \n",
    "                                                 # the time. \n",
    "                    mean_subtract = False )\n",
    "    \n",
    "    # add first convolutional layer\n",
    "    net.add_layer ( type = \"conv_pool\",\n",
    "                    origin = \"input\",\n",
    "                    id = \"conv_pool_1\",\n",
    "                    num_neurons = 20,\n",
    "                    filter_size = (5,5),\n",
    "                    pool_size = (2,2),\n",
    "                    activation = 'maxout(2,2)',\n",
    "                    # regularize = True,\n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    net.add_layer ( type = \"conv_pool\",\n",
    "                    origin = \"conv_pool_1\",\n",
    "                    id = \"conv_pool_2\",\n",
    "                    num_neurons = 50,\n",
    "                    filter_size = (3,3),\n",
    "                    pool_size = (2,2),\n",
    "                    activation = 'relu',\n",
    "                    # regularize = True,\n",
    "                    verbose = verbose\n",
    "                    )      \n",
    "\n",
    "\n",
    "    net.add_layer ( type = \"dot_product\",\n",
    "                    origin = \"conv_pool_2\",\n",
    "                    id = \"dot_product_1\",\n",
    "                    num_neurons = 1250,\n",
    "                    activation = 'relu',\n",
    "                    # regularize = True,\n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    net.add_layer ( type = \"dot_product\",\n",
    "                    origin = \"dot_product_1\",\n",
    "                    id = \"dot_product_2\",\n",
    "                    num_neurons = 1250,                    \n",
    "                    activation = 'relu',  \n",
    "                    # regularize = True,    \n",
    "                    verbose = verbose\n",
    "                    ) \n",
    "    \n",
    "    net.add_layer ( type = \"classifier\",\n",
    "                    id = \"softmax\",\n",
    "                    origin = \"dot_product_2\",\n",
    "                    num_classes = 10,\n",
    "                    # regularize = True,\n",
    "                    activation = 'softmax',\n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    net.add_layer ( type = \"objective\",\n",
    "                    id = \"obj\",\n",
    "                    origin = \"softmax\",\n",
    "                    objective = \"nll\",\n",
    "                    datastream_origin = 'data', \n",
    "                    regularization = regularization,                \n",
    "                    verbose = verbose\n",
    "                    )\n",
    "                    \n",
    "    learning_rates = (0.05, .0001, 0.001)  \n",
    "    net.pretty_print()  \n",
    "    # draw_network(net.graph, filename = 'lenet.png')   \n",
    "\n",
    "    net.cook()\n",
    "\n",
    "    net.train( epochs = (20, 20), \n",
    "               validate_after_epochs = 1,\n",
    "               training_accuracy = True,\n",
    "               learning_rates = learning_rates,               \n",
    "               show_progress = True,\n",
    "               early_terminate = True,\n",
    "               patience = 2,\n",
    "               verbose = verbose)\n",
    "\n",
    "    print(net.test(verbose = verbose))\n",
    "data = cook_mnist()\n",
    "dataset = data.dataset_location()\n",
    "lenet5 ( dataset, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
