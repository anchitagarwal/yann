{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoders\n",
    "Auto Encoders are unsupervised learning technique where we conert an image into code word and the image can be regenerated from the code word. Auto encoder is a combination of two convolutional networks, The encoder and The decoder. Encoder is a convolutional network which gives the code word at the end. decoder is a network with deconvolution layers and it converts the codeword to image by using deconvolution layers. Deconvolution layers have the transposed weights of convolution layers and they use fractional striding to give the effect of unpooling. \n",
    "\n",
    "An example Auto Encoder can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " creating a new dataset to run through\n",
      ". Setting up dataset \n",
      ".. setting up skdata\n",
      "... Importing mnist from skdata\n",
      ".. setting up dataset\n",
      ".. training data\n",
      ".. validation data \n",
      ".. testing data \n",
      ". Dataset 96755 is created.\n",
      ". Time taken is 0.600982 seconds\n",
      ". Initializing the network\n",
      ".. Setting up the datastream\n",
      ".. Setting up the visualizer\n",
      ".. Setting up the optimizer\n",
      ".. Adding input layer input\n",
      ".. Adding conv_pool layer conv\n",
      ".. Adding flatten layer flatten\n",
      ".. Adding dot_product layer hidden-encoder\n",
      ".. Adding dot_product layer encoder\n",
      ".. Adding dot_product layer decoder\n",
      ".. Adding dot_product layer hidden-decoder\n",
      ".. Adding unflatten layer unflatten\n",
      ".. Adding deconv layer deconv\n",
      ".. Adding merge layer merge\n",
      ".. Adding objective layer obj\n",
      ".. Cooking the network\n",
      ".. Setting up the resultor\n",
      ".. All checks complete, cooking continues\n",
      ".. This method will be deprecated with the implementation of a visualizer,also this works only for tree-like networks. This will cause errors in printing DAG-style networks.\n",
      " |-\n",
      " |-\n",
      " |-\n",
      " |- id: input\n",
      " |-=================------------------\n",
      " |- type: input\n",
      " |- output shape: (500, 1, 28, 28)\n",
      " |------------------------------------\n",
      "          |-\n",
      "          |-\n",
      "          |-\n",
      "          |- id: conv\n",
      "          |-=================------------------\n",
      "          |- type: conv_pool\n",
      "          |- output shape: (500, 20, 24, 24)\n",
      "          |- batch norm is OFF\n",
      "          |------------------------------------\n",
      "          |- filter size [5 X 5]\n",
      "          |- pooling size [1 X 1]\n",
      "          |- stride size [1 X 1]\n",
      "          |- input shape [28 28]\n",
      "          |- input number of feature maps is 1\n",
      "          |------------------------------------\n",
      "          |        |-\n",
      "          |        |-\n",
      "          |        |-\n",
      "          |        |- id: flatten\n",
      "          |        |-=================------------------\n",
      "          |        |- type: flatten\n",
      "          |        |- output shape: (500, 11520)\n",
      "          |        |------------------------------------\n",
      "          |                 |-\n",
      "          |                 |-\n",
      "          |                 |-\n",
      "          |                 |- id: hidden-encoder\n",
      "          |                 |-=================------------------\n",
      "          |                 |- type: dot_product\n",
      "          |                 |- output shape: (500, 1200)\n",
      "          |                 |- batch norm is OFF\n",
      "          |                 |------------------------------------\n",
      "          |                          |-\n",
      "          |                          |-\n",
      "          |                          |-\n",
      "          |                          |- id: encoder\n",
      "          |                          |-=================------------------\n",
      "          |                          |- type: dot_product\n",
      "          |                          |- output shape: (500, 128)\n",
      "          |                          |- batch norm is OFF\n",
      "          |                          |------------------------------------\n",
      "          |                                   |-\n",
      "          |                                   |-\n",
      "          |                                   |-\n",
      "          |                                   |- id: decoder\n",
      "          |                                   |-=================------------------\n",
      "          |                                   |- type: dot_product\n",
      "          |                                   |- output shape: (500, 1200)\n",
      "          |                                   |- batch norm is OFF\n",
      "          |                                   |------------------------------------\n",
      "          |                                            |-\n",
      "          |                                            |-\n",
      "          |                                            |-\n",
      "          |                                            |- id: hidden-decoder\n",
      "          |                                            |-=================------------------\n",
      "          |                                            |- type: dot_product\n",
      "          |                                            |- output shape: (500, 11520)\n",
      "          |                                            |- batch norm is OFF\n",
      "          |                                            |------------------------------------\n",
      "          |                                                     |-\n",
      "          |                                                     |-\n",
      "          |                                                     |-\n",
      "          |                                                     |- id: unflatten\n",
      "          |                                                     |-=================------------------\n",
      "          |                                                     |- type: unflatten\n",
      "          |                                                     |- output shape: (500, 20, 24, 24)\n",
      "          |                                                     |------------------------------------\n",
      "          |                                                              |-\n",
      "          |                                                              |-\n",
      "          |                                                              |-\n",
      "          |                                                              |- id: deconv\n",
      "          |                                                              |-=================------------------\n",
      "          |                                                              |- type: deconv\n",
      "          |                                                              |- output shape: (500, 1, 28, 28)\n",
      "          |                                                              |------------------------------------\n",
      "          |                                                              |- filter size [5 X 5]\n",
      "          |                                                              |- stride size [1 X 1]\n",
      "          |                                                              |- input shape [24 24]\n",
      "          |                                                              |- input number of feature maps is 20\n",
      "          |                                                              |------------------------------------\n",
      "          |                                                                       |-\n",
      "          |                                                                       |-\n",
      "          |                                                                       |-\n",
      "          |                                                                       |- id: merge\n",
      "          |                                                                       |-=================------------------\n",
      "          |                                                                       |- type: merge\n",
      "          |                                                                       |- output shape: (1,)\n",
      "          |                                                                       |------------------------------------\n",
      "          |                                                                                |-\n",
      "          |                                                                                |-\n",
      "          |                                                                                |-\n",
      "          |                                                                                |- id: obj\n",
      "          |                                                                                |-=================------------------\n",
      "          |                                                                                |- type: objective\n",
      "          |                                                                                |- output shape: (1,)\n",
      "          |                                                                                |------------------------------------\n",
      "          |-\n",
      "          |-\n",
      "          |-\n",
      "          |- id: merge\n",
      "          |-=================------------------\n",
      "          |- type: merge\n",
      "          |- output shape: (1,)\n",
      "          |------------------------------------\n",
      "                   |-\n",
      "                   |-\n",
      "                   |-\n",
      "                   |- id: obj\n",
      "                   |-=================------------------\n",
      "                   |- type: objective\n",
      "                   |- output shape: (1,)\n",
      "                   |------------------------------------\n",
      ". Training\n",
      ". \n",
      "\n",
      ".. Epoch: 0 Era: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:13                                                 \n",
      "| validation  100% Time: 0:00:07                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.877471\n",
      "... Learning Rate       : 0.00999999977648\n",
      "... Momentum            : 0.649999976158\n",
      ". \n",
      "\n",
      ".. Epoch: 1 Era: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:13                                                 \n",
      "| validation  100% Time: 0:00:07                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.856174\n",
      "... Learning Rate       : 0.00949999969453\n",
      "... Momentum            : 0.659999966621\n",
      ". \n",
      "\n",
      ".. Epoch: 2 Era: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:13                                                 \n",
      "| validation  100% Time: 0:00:07                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.84899\n",
      "... Learning Rate       : 0.00902500003576\n",
      "... Momentum            : 0.669999957085\n",
      ". \n",
      "\n",
      ".. Epoch: 3 Era: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:13                                                 \n",
      "| validation  100% Time: 0:00:07                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.860179\n",
      "... Learning Rate       : 0.00857375003397\n",
      "... Momentum            : 0.679999947548\n",
      ".. Patience ran out lowering learning rate.\n",
      ". \n",
      "\n",
      ".. Epoch: 4 Era: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:13                                                 \n",
      "| validation  100% Time: 0:00:07                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.486874\n",
      "... Learning Rate       : 0.000814506202005\n",
      "... Momentum            : 0.689999997616\n",
      ".. Patience ran out lowering learning rate.\n",
      ". \n",
      "\n",
      ".. Epoch: 5 Era: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:13                                                 \n",
      "| validation  100% Time: 0:00:07                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.593437\n",
      "... Learning Rate       : 7.73780848249e-05\n",
      "... Momentum            : 0.699999928474\n",
      ".. Patience ran out lowering learning rate.\n",
      ". \n",
      "\n",
      ".. Epoch: 6 Era: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:13                                                 \n",
      "| validation  100% Time: 0:00:07                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.616476\n",
      "... Learning Rate       : 7.3509181675e-06\n",
      "... Momentum            : 0.709999978542\n",
      ".. Patience ran out lowering learning rate.\n",
      ". \n",
      "\n",
      ".. Epoch: 7 Era: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:13                                                 \n",
      "| validation  100% Time: 0:00:07                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.616585\n",
      "... Learning Rate       : 6.98337203175e-07\n",
      "... Momentum            : 0.719999969006\n",
      ".. Patience ran out lowering learning rate.\n",
      ". \n",
      "\n",
      ".. Epoch: 8 Era: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:13                                                 \n",
      "| validation  100% Time: 0:00:07                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.616599\n",
      "... Learning Rate       : 6.63420323121e-08\n",
      "... Momentum            : 0.730000019073\n",
      ".. Patience ran out lowering learning rate.\n",
      ". \n",
      "\n",
      ".. Epoch: 9 Era: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:13                                                 \n",
      "| validation  100% Time: 0:00:07                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.616601\n",
      "... Learning Rate       : 6.30249274991e-09\n",
      "... Momentum            : 0.740000009537\n",
      ".. Patience ran out lowering learning rate.\n",
      ".. Learning rate was already lower than specified. Not changing it.\n",
      ".. Old learning rate was :5.98736782376e-10\n",
      ".. Was trying to change to: 0.001\n",
      ". \n",
      "\n",
      ".. Epoch: 10 Era: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| training  100% Time: 0:00:13                                                 \n",
      "| validation  100% Time: 0:00:07                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Cost                : 0.616601\n",
      "... Learning Rate       : 5.98736782376e-10\n",
      "... Momentum            : 0.749999940395\n",
      ".. Early stopping\n",
      ".. Training complete.Took 10.76886435 minutes\n"
     ]
    }
   ],
   "source": [
    "from yann.network import network\n",
    "def convolutional_autoencoder ( dataset= None, verbose = 1 ):\n",
    "    \"\"\"\n",
    "    This function is a demo example of a deep convolutional autoencoder. \n",
    "    This is an example code. You should study this code rather than merely run it.  \n",
    "    This is also an example for using the deconvolutional layer or the transposed fractional stride\n",
    "    convolutional layers.\n",
    "\n",
    "    Args: \n",
    "        dataset: Supply a dataset.    \n",
    "        verbose: Similar to the rest of the dataset.\n",
    "    \"\"\"\n",
    "    dataset_params  = {\n",
    "                            \"dataset\"   : dataset,\n",
    "                            \"type\"      : 'x',\n",
    "                            \"id\"        : 'data'\n",
    "                    }\n",
    "\n",
    "    visualizer_params = {\n",
    "                    \"root\"       : '.',\n",
    "                    \"frequency\"  : 1,\n",
    "                    \"sample_size\": 32,\n",
    "                    \"rgb_filters\": False,\n",
    "                    \"debug_functions\" : False,\n",
    "                    \"debug_layers\": True,  \n",
    "                    \"id\"         : 'main'\n",
    "                        }  \n",
    "                      \n",
    "    # intitialize the network    \n",
    "    optimizer_params =  {        \n",
    "                \"momentum_type\"       : 'nesterov',             \n",
    "                \"momentum_params\"     : (0.65, 0.95, 30),      \n",
    "                \"regularization\"      : (0.0001, 0.0001),       \n",
    "                \"optimizer_type\"      : 'rmsprop',                \n",
    "                \"id\"                  : \"main\"\n",
    "                    }\n",
    "    net = network(   borrow = True,\n",
    "                     verbose = verbose )                       \n",
    "\n",
    "    net.add_module ( type = 'datastream', \n",
    "                     params = dataset_params,\n",
    "                     verbose = verbose )\n",
    "    \n",
    "    net.add_module ( type = 'visualizer',\n",
    "                     params = visualizer_params,\n",
    "                     verbose = verbose \n",
    "                    ) \n",
    "    net.add_module ( type = 'optimizer',\n",
    "                     params = optimizer_params,\n",
    "                     verbose = verbose )\n",
    "    # add an input layer \n",
    "    net.add_layer ( type = \"input\",\n",
    "                    id = \"input\",\n",
    "                    verbose = verbose, \n",
    "                    origin = 'data', # if you didnt add a dataset module, now is \n",
    "                                                 # the time. \n",
    "                    mean_subtract = True )\n",
    "\n",
    "    \n",
    "    net.add_layer ( type = \"conv_pool\",\n",
    "                    origin = \"input\",\n",
    "                    id = \"conv\",\n",
    "                    num_neurons = 20,\n",
    "                    filter_size = (5,5),\n",
    "                    pool_size = (1,1),\n",
    "                    activation = 'tanh',\n",
    "                    regularize = True,   \n",
    "                    #stride = (2,2),                          \n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    net.add_layer ( type = \"flatten\",\n",
    "                    origin = \"conv\",\n",
    "                    id = \"flatten\",\n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    net.add_layer ( type = \"dot_product\",\n",
    "                    origin = \"flatten\",\n",
    "                    id = \"hidden-encoder\",\n",
    "                    num_neurons = 1200,\n",
    "                    activation = 'tanh',\n",
    "                    dropout_rate = 0.5,                    \n",
    "                    regularize = True,\n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    net.add_layer ( type = \"dot_product\",\n",
    "                    origin = \"hidden-encoder\",\n",
    "                    id = \"encoder\",\n",
    "                    num_neurons = 128,\n",
    "                    activation = 'tanh',\n",
    "                    dropout_rate = 0.5,                        \n",
    "                    regularize = True,\n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    net.add_layer ( type = \"dot_product\",\n",
    "                    origin = \"encoder\",\n",
    "                    id = \"decoder\",\n",
    "                    num_neurons = 1200,\n",
    "                    activation = 'tanh',\n",
    "                    input_params = [net.dropout_layers['encoder'].w.T, None],\n",
    "                    # Use the same weights but transposed for decoder. \n",
    "                    learnable = False,                    \n",
    "                    # because we don't want to learn the weights of somehting already used in \n",
    "                    # an optimizer, when reusing the weights, always use learnable as False   \n",
    "                    dropout_rate = 0.5,                                         \n",
    "                    verbose = verbose\n",
    "                    )           \n",
    "\n",
    "    net.add_layer ( type = \"dot_product\",\n",
    "                    origin = \"decoder\",\n",
    "                    id = \"hidden-decoder\",\n",
    "                    num_neurons = net.layers['flatten'].output_shape[1],\n",
    "                    activation = 'tanh',\n",
    "                    input_params = [net.dropout_layers['hidden-encoder'].w.T, None],\n",
    "                    # Use the same weights but transposed for decoder. \n",
    "                    learnable = False,                    \n",
    "                    # because we don't want to learn the weights of somehting already used in \n",
    "                    # an optimizer, when reusing the weights, always use learnable as False    \n",
    "                    dropout_rate = 0.5,                                        \n",
    "                    verbose = verbose\n",
    "                    )                                            \n",
    "\n",
    "    net.add_layer ( type = \"unflatten\",\n",
    "                    origin = \"hidden-decoder\",\n",
    "                    id = \"unflatten\",\n",
    "                    shape = (net.layers['conv'].output_shape[2],\n",
    "                             net.layers['conv'].output_shape[3],\n",
    "                             20),\n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    net.add_layer ( type = \"deconv\",\n",
    "                    origin = \"unflatten\",\n",
    "                    id = \"deconv\",\n",
    "                    num_neurons = 20,\n",
    "                    filter_size = (5,5),\n",
    "                    pool_size = (1,1),\n",
    "                    output_shape = (28,28,1),\n",
    "                    activation = 'tanh',\n",
    "                    input_params = [net.dropout_layers['conv'].w, None],        \n",
    "                    learnable = False,              \n",
    "                    #stride = (2,2),\n",
    "                    verbose = verbose\n",
    "                    )\n",
    "\n",
    "    # We still need to learn the newly created biases in the decoder layer, so add them to the \n",
    "    # Learnable parameters list before cooking\n",
    "\n",
    "    net.active_params.append(net.dropout_layers['hidden-decoder'].b)\n",
    "    net.active_params.append(net.dropout_layers['decoder'].b)    \n",
    "    net.active_params.append(net.dropout_layers['deconv'].b)\n",
    "    \n",
    "\n",
    "    net.add_layer ( type = \"merge\",\n",
    "                    origin = (\"input\",\"deconv\"),\n",
    "                    id = \"merge\",\n",
    "                    layer_type = \"error\",\n",
    "                    error = \"rmse\",\n",
    "                    verbose = verbose)\n",
    "\n",
    "    net.add_layer ( type = \"objective\",\n",
    "                    id = \"obj\",\n",
    "                    origin = \"merge\", # this is useless anyway.\n",
    "                    layer_type = 'value',\n",
    "                    objective = net.layers['merge'].output,\n",
    "                    datastream_origin = 'data', \n",
    "                    verbose = verbose\n",
    "                    )          \n",
    "\n",
    "    learning_rates = (0.04, 0.0001, 0.00001)  \n",
    "    net.cook( objective_layers = ['obj'],\n",
    "              datastream = 'data',\n",
    "              learning_rates = learning_rates,\n",
    "              verbose = verbose\n",
    "              )\n",
    "\n",
    "    # from yann.utils.graph import draw_network\n",
    "    # draw_network(net.graph, filename = 'autoencoder.png')    \n",
    "    net.pretty_print()\n",
    "    net.train( epochs = (10, 10), \n",
    "               validate_after_epochs = 1,\n",
    "               training_accuracy = True,\n",
    "               show_progress = True,\n",
    "               early_terminate = True,\n",
    "               verbose = verbose)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    print \" creating a new dataset to run through\"\n",
    "    from yann.special.datasets import cook_mnist_normalized_zero_mean as cook_mnist  \n",
    "    data = cook_mnist (verbose = 2)\n",
    "    dataset = data.dataset_location()\n",
    "\n",
    "    convolutional_autoencoder ( dataset , verbose = 2 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
